# Presentation

Clouds play a crucial role in earth temperature regulation. They can block light and heat from the Sun, making Earth’s temperature cooler. However, some heat from the Sun does get down to Earth. Clouds can trap that heat from the Sun so that at night, when there’s no sunlight, the temperature won't be too low.

Just as clouds affect climate, changes in the climate affect clouds. This relationship is known as cloud-climate feedback. Climate scientists predict that as Earth’s climate warms, there will also be fewer clouds to cool it down.
Scientists who try to predict changes in the climate are trying to understand the complex role of clouds in our atmosphere as they figure out how Earth is changing.

Global Climate Models (GCMs) are one of the most important tools available to understand and anticipate the consequences of climate change. One of the key physical principles these models rely on is the Earth’s energy balance: in short, the difference between how much energy the earth receives and how much it emits. It is therefore crucial to model clouds accurately as they both reflect energy coming to the Earth and the infrared radiations it radiates. 

However, as physical processes at play in cloud composition and evolution typically range from `1e−6` to `1e6` m, direct simulation of their behavior can consume up to 20% of a GCM’s computations - depending on their time and spatial scales

This work is an extension of [1], proposing a data-driven approach to contribute to cloud modeling, focusing on one of the main features used in energy balance calculations: reflectance fields
It uses Generative Adversarial Networks to generate these reflectance fields conditioned on meteorological variables. It suggests using these generated images to extract important cloud parameters such as optical depth.


# Data

The data come from the <i> MODerate resolution Imaging Spectroradiometer (MODIS) </i>

The clouds images should be contained in a single folder that we will reference as the variable `DATA_FOLDER_PATH`. It should contain 4 files:

- train.npy : containing the images in numpy format
- meto.npy : containing the meteorological variables associated with train.npy in numpy format
- files.npy : containing the names of the images in train.npy in numpy format
- metos_stats.npy : containing the means and standard deviations of each (8) meteorological variables


# Installation

## Using Singularity

All required packages are installed in the singularity image provided in ... To launch an interactive session with that image, run the following command:

    singularity shell --nv --bind "$DATA_FOLDER_PATH" $SINGULARITY_IMG_PATH

You may have to bind folders like ",/etc/pki/tls/certs/,/etc/pki/ca-trust/extracted/pem"

## Using pip

The following command will install the required packages listed in "requirements.txt"

    pip install -r requirements.txt

# Train a GAN 

The following command

	python infogan.py --o output_dir -c config_file.yaml

will train a GAN defined in "config_file.yaml" and output the associated files in "output_dir"

## Simple DCGAN

You can use the configuration file `dcgan_film01.yaml`

## InfoGAN

You can use the configuration file `default_training_config.yaml`

## FiLM (Feature wise Linear Modulation)

To use FiLM, you have to modify the field `film_layers` in the section `model` and specify the indices of the layers you want to FiLM separated by `a`.
For example, if you want to FiLM the first and second layers (corresponding to indices 0 and 1), you should have in your configuration file `film_layers: "0a1"`. An example can be found in `dcgan_film01.yaml`

## Other options

# Meteorological variables (Metos) Regressor

For a given `(metos,cloud)` pair in the dataset, a metos regressor takes as input the `cloud` image and estimates the associated `metos`.

Evaluated on images generated by a GAN, the metos regressor should output values correlated with the metos used to generate the latter image, thus providing a quantitative measure of conditional image generation.

## Train a metos regressor

The following command

	python metos_regressor.py --o $EXP_DIR -c config_file.yaml -g gan_model_path.pth --train True

will train a metos regressor defined in "config_file.yaml" and output the associated files (metos regressor weights, spearman correlation etc...) in "$EXP_DIR". It will also evaluate the GAN (whose weights are in `gan_model_path.pth`) on the test partition of the dataset. To only evaluate the latter GAN, you just have to remove `--train True`. If you want to predict the mean of the 8 metos rather than each of them individually, you can use the option `-n 1`

The evaluation of the GAN will produce, inside `$EXP_DIR`, 3 categories of files:

1. <b>Real versus predicted (`real_predicted`) files</b> : they correspond to a sanity check. Metos regression as a metric is useful only if the metos regression model is good enough. A necessary condition for it is that given real images from the real dataset, the metos regressor should predict values that correlate with the original metos associated with the real images. The `real_predicted_correlation.csv` file contains the Spearman correlations between the real metos and the predicted values by the metos regressor on the test set. For a meto of index `i`, the file `real_predicted_i.png` represents a scatter plot with the real meto on the `x-axis` and the predicted one on the `y-axis`

2. <b>Generated versus real (`generated_real`) files</b> : These are the most important files. They compare the original metos in the dataset with the metos predicted by the metos regressor on the images generated by the GAN. Each image `gan_img` produced by a GAN is generated using the real metos `real_metos`. The metos regressor try to recover the latter by producing the estimate `predicted_metos`. The `real_generated_correlation.csv` file contains the Spearman correlations between `real_metos` and `predicted_metos` on the test set. For a meto of index `i`, the file `generated_real_i.png` represents a scatter plot with `predicted_metos` on the `x-axis` and `real_metos` on the `y-axis`

3. <b>Generated versus predicted (`generated_predicted`) files</b> . Same as before but instead of using `real_metos`, we use the metos predicted on the <i> real </i> images by the metos regressor

This evaluation also produces a comparison between the Fast Fourrier Transforms (FFT) of the real images and the FFT of the images generated by the GAN. They are stored in the `fft` directory inside `$EXP_DIR`

## Do model selection using the metos regressor

The metos regressor can also be used to select the best model out of several checkpoints saved during the training of a GAN. It thus permits to select a model based on a quantitave metric (rather than just human visual evaluation). To do so, you can use the following command 

    python3 metos_regressor.py -o $EXP_DIR -v $TOP_K -m $MODELS_DIR


- `$EXP_DIR` refers to the same directory as stated in the previous section. It should contain the weights of the trained metos regressor (`state_best.pt`)
- `$TOP_K` represent the number `k` of a "top k" procedure. Rather than just indicating the best model, we can output the `$TOP_K` best models
- `$MODELS_DIR` is the directory where the model checkpoints are saved. It should contains files like `state_x.pt`, `state_2*x.pt` , etc... where `x` is the checkpointing frequence (the `save_every` argument in the `train` section of the configuration file)

Since we need a single metric to compare the different models, the 8 correlations (one per metos) are aggregated using a metric that can be specified using the `--summary_metric METRIC` option. `METRIC` can be `mean (default), max, min, or median`.

If we want to use just a subset of the metos correlations rather than all 8 of them, the specific metos indices of interest can be specified using the `--metos_indices INDICES` option. `INDICES` is a string made of indices separated by `a` (for "and"). For example, if we only want to use the metos corresponding to the indices 1 and 3, we just need to use `--metos_indices 1a3`.

The models are selected based on their performance on the validation set and their performance on the test set are reported in the output of the command.

This command will produce two sets of directories:

1. <b> `state_x` </b> directories : they contain the output of the evaluation (on the validation set) explained in the previous section for each `state_x.pt` model

2. <b> `top_i_test_state_x` </b> :  they contain the output of the evaluation (<i> on the test set </i>) explained in the previous section for the `$TOP_K` models. `i` represent the rank of the model i.e. `i = 1` is the directory of the best model, `i = 2` the one associated with the second best model etc...

# References

[1] T. Yuan, H. Song, D. Hall, V. Schmidt, K. Sankaran, and Y. Bengio. Artificial intelligence
based cloud distributor (ai-cd): probing clouds with generative adversarial networks. AGU Fall
Meeting 2019, 2019.
