# -----------------------
# -----    Model    -----
# -----------------------
model:
    noise_dim: 100
    n_blocks: 5 
    depth_increase_factor: 2
    generator: "dcgan" # Use DCGan or defaut Generator
    discriminator: "dcgan" # Use DCGan or default (multiscale) discriminator
    ngf: 64
    ndf: 64
    nc: 1
    film_layers: "" #Layers to FiLM, separated by a "a"
    spectral_norm: true
    infogan: true
    num_dis_c : 1 # InfoGAN - Number of discrete latent code.
    dis_c_dim : 10 # InfoGAN - Dimension of discrete latent code.
    num_con_c : 0 # InfoGAN - Number of continuous latent code.
    num_z : 90 # InfoGAN - Dimension of iicompressible noise.
    concat_noise_metos: true
    predict_metos: true
    
# ------------------------------
# -----    Train Params    -----
# ------------------------------
train:
    batch_size: 780 # 64 -> 780
    early_break_epoch: 0 # Break an epoch loop after early_break_epoch steps in this epoch
    init_chkpt_dir: null # can initialize from a checkpoint directory
    init_chkpt_step: "latest" # can give path to saved checkpoint
    init_keys: ["g.down*", "models.d*"]
    lambda_gan: 1 # Gan loss scaling constant
    lambda_L: 2 # Matching loss scaling constant
    lambda_infogan : 1 #Mutual Information Maximization constant
    lambda_metos : 1
    lr_d: 0.001 # Discriminator's learning rate
    lr_g: 0.01 # Generator's learning rate
    matching_loss: l1 # Which matching loss to use: l2 | l1 | weighted
    n_epochs: 10001 # How many training epochs
    num_D_accumulations: 1 # How many gradients to accumulate in current batch (different generator predictions) before doing one discriminator optimization step
    offline_losses_steps: 50 # how often to log the losses with no comet logs
    optimizer: "extraadam" # one of [adam, extrasgd, extraadam]
    save_every: 100 # How often to save  the model's weights
    use_extragradient_optimizer: true # >>DEPRECATED: use optimizer instead<< use ExtragradientSGD or Adam(betas=(0.5, 0.999))
    n_infer: 1
    log_every: 10 
    feature_extractor_loss: false
    feature_extractor_model: "resnet18"
    infer_every: 100 # How often to infer validation images
    use_wandb: true
# ------------------------
# -----  Validation  -----
# ------------------------
val:
    #val_ids: [2012089.1520_index_1344_index_0256, 2012269.2145_index_0320_index_0832, 2012360.1935_index_0384_index_0704]
    val_ids: [2012181.1410_index_0832_index_0896, 2012164.1325_index_1344_index_0768, 2012347.1510_index_0384_index_0832] #cluster 1
    #val_ids: [2012269.2145_index_0320_index_0832, 2012360.1935_index_0384_index_0704, 2012089.1520_index_1344_index_0256]
    infer_every: 100 # How often to infer validation images
    store_images: false # Do you want to write inferred images to disk
    nb_of_inferences: 1 # no of inferences (generated imgs) per validation sample (input + real_img)
    set_size: 64
    n_infer: 1

# ---------------------------
# -----    Data Conf    -----
# ---------------------------
data:
    path: "/scratch/jassiene/data/low_clouds/full_dataset" # Where's the data?
    cloud_type: "local" # global = earth, local = low_clouds
    preprocessed_data_path: null # If you set this path to something != null, it will override the "data" path
    num_workers: 12 # How many workers for the dataloader
    with_stats: true # Normalize with stats? Computed before the training loop if no using preprocessed data
    load_limit: 1000 # Limit the number of samples per epoch | -1 to disable
    squash_channels: false # If set to True, don't forget to change model.Cin from 44 to 8
    crop_to_inner_square: false # crop metos and imgs to the size of the real_img's inner square
    clip_reflectance: 0.9 # set to -1 not to use this transform
    noq: 300 # if it's not null quantize the data to no_of_quantiles
    image_size: 64
    normalize: true
